# Tistory Blog Crawler

í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ í†µí•´ ê²Œì‹œê¸€, ëŒ“ê¸€, ì¢‹ì•„ìš” ìˆ˜ ë“±ì„ í¬ë¡¤ë§í•˜ëŠ” Python ë„êµ¬ì…ë‹ˆë‹¤.

## ğŸš€ ì£¼ìš” ê¸°ëŠ¥

- **í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰**: í‹°ìŠ¤í† ë¦¬ ê²€ìƒ‰ì„ í†µí•œ ê´€ë ¨ ê²Œì‹œê¸€ ìˆ˜ì§‘
- **ê¸°ê°„ í•„í„°ë§**: ì§€ì •ëœ ë‚ ì§œ ë²”ìœ„ ë‚´ì˜ ê²Œì‹œê¸€ë§Œ ìˆ˜ì§‘
- **ìƒì„¸ ì •ë³´ ìˆ˜ì§‘**: 
  - ê²Œì‹œê¸€ ì œëª©, ë‚´ìš©, URL
  - ë¸”ë¡œê·¸ ì´ë¦„ ë° URL
  - ì¢‹ì•„ìš”(ê³µê°) ìˆ˜
  - ì¡°íšŒìˆ˜
  - ëŒ“ê¸€ ìˆ˜ ë° ëŒ“ê¸€ ë‚´ìš©
  - ì‘ì„± ë‚ ì§œ
- **ë‹¤ì–‘í•œ ì¶œë ¥ í˜•ì‹**: CSV, JSON íŒŒì¼ë¡œ ê²°ê³¼ ì €ì¥
- **ë‘ ê°€ì§€ í¬ë¡¤ë§ ë°©ì‹**: 
  - ê¸°ë³¸ requests/BeautifulSoup ë°©ì‹
  - Seleniumì„ ì´ìš©í•œ ë™ì  ì½˜í…ì¸  ì§€ì›

## ğŸ“¦ ì„¤ì¹˜ ë°©ë²•

### 1. ì €ì¥ì†Œ í´ë¡ 
```bash
git clone <repository-url>
cd tistory-crawler
```

### 2. ì˜ì¡´ì„± ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

### 3. Chrome ë¸Œë¼ìš°ì € ì„¤ì¹˜ (Selenium ì‚¬ìš© ì‹œ)
Selenium ë²„ì „ì„ ì‚¬ìš©í•˜ë ¤ë©´ Chrome ë¸Œë¼ìš°ì €ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

## ğŸ–¥ï¸ ì‚¬ìš© ë°©ë²•

### ê¸°ë³¸ í¬ë¡¤ëŸ¬ ì‚¬ìš©

```python
from tistory_crawler import TistoryCrawler

# í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
crawler = TistoryCrawler(delay=1.0)

# í¬ë¡¤ë§ ì‹¤í–‰
posts = crawler.crawl_keyword_posts(
    keyword="íŒŒì´ì¬",
    start_date="2024-01-01",
    end_date="2024-01-31",
    max_pages=5,
    include_details=True
)

# ê²°ê³¼ ì €ì¥
crawler.save_to_csv(posts, "python_posts.csv")
crawler.save_to_json(posts, "python_posts.json")
```

### Selenium í¬ë¡¤ëŸ¬ ì‚¬ìš©

```python
from tistory_selenium_crawler import TistorySeleniumCrawler

# í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
crawler = TistorySeleniumCrawler(headless=True, delay=2.0)

try:
    # í¬ë¡¤ë§ ì‹¤í–‰
    posts = crawler.crawl_keyword_posts(
        keyword="ë¸”ë¡œê·¸",
        start_date="2024-01-01",
        max_pages=3,
        include_details=True
    )
    
    # ê²°ê³¼ ì €ì¥
    crawler.save_to_csv(posts)
    
finally:
    # WebDriver ì •ë¦¬
    crawler.driver.quit()
```

### ëª…ë ¹í–‰ì—ì„œ ì‹¤í–‰

```bash
# ê¸°ë³¸ í¬ë¡¤ëŸ¬
python tistory_crawler.py

# Selenium í¬ë¡¤ëŸ¬
python tistory_selenium_crawler.py

# ì‚¬ìš© ì˜ˆì œ
python example_usage.py
```

## ğŸ“Š ì¶œë ¥ ë°ì´í„° í˜•ì‹

### CSV íŒŒì¼ êµ¬ì¡°
| ì»¬ëŸ¼ëª… | ì„¤ëª… |
|--------|------|
| title | ê²Œì‹œê¸€ ì œëª© |
| url | ê²Œì‹œê¸€ URL |
| blog_name | ë¸”ë¡œê·¸ ì´ë¦„ |
| blog_url | ë¸”ë¡œê·¸ URL |
| date | ì‘ì„± ë‚ ì§œ |
| content | ê²Œì‹œê¸€ ë‚´ìš© |
| like_count | ì¢‹ì•„ìš”(ê³µê°) ìˆ˜ |
| view_count | ì¡°íšŒìˆ˜ |
| comment_count | ëŒ“ê¸€ ìˆ˜ |
| preview | ë¯¸ë¦¬ë³´ê¸° í…ìŠ¤íŠ¸ |
| comment_author | ëŒ“ê¸€ ì‘ì„±ì |
| comment_content | ëŒ“ê¸€ ë‚´ìš© |
| comment_date | ëŒ“ê¸€ ì‘ì„± ë‚ ì§œ |
| crawled_at | í¬ë¡¤ë§ ì‹œê°„ |

### JSON íŒŒì¼ êµ¬ì¡°
```json
[
  {
    "title": "ê²Œì‹œê¸€ ì œëª©",
    "url": "https://blog.tistory.com/123",
    "blog_name": "ë¸”ë¡œê·¸ ì´ë¦„",
    "blog_url": "https://blog.tistory.com",
    "date": "2024.01.15",
    "content": "ê²Œì‹œê¸€ ì „ì²´ ë‚´ìš©",
    "like_count": 10,
    "view_count": 150,
    "comment_count": 5,
    "comments": [
      {
        "author": "ëŒ“ê¸€ ì‘ì„±ì",
        "content": "ëŒ“ê¸€ ë‚´ìš©",
        "date": "2024.01.16"
      }
    ],
    "preview": "ê²Œì‹œê¸€ ë¯¸ë¦¬ë³´ê¸°",
    "crawled_at": "2024-01-20T10:30:00"
  }
]
```

## âš™ï¸ ì„¤ì • ì˜µì…˜

### TistoryCrawler ì˜µì…˜
- `delay`: ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ, ê¸°ë³¸ê°’: 1.0)

### TistorySeleniumCrawler ì˜µì…˜
- `headless`: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
- `delay`: ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ, ê¸°ë³¸ê°’: 2.0)

### crawl_keyword_posts ë§¤ê°œë³€ìˆ˜
- `keyword`: ê²€ìƒ‰í•  í‚¤ì›Œë“œ (í•„ìˆ˜)
- `start_date`: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹, ì„ íƒì‚¬í•­)
- `end_date`: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹, ì„ íƒì‚¬í•­)
- `max_pages`: ìµœëŒ€ í¬ë¡¤ë§ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 10)
- `include_details`: ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)

## ğŸ”§ ê³ ê¸‰ ì‚¬ìš©ë²•

### ë‚ ì§œ í•„í„°ë§
```python
from datetime import datetime, timedelta

# ìµœê·¼ 30ì¼ê°„ì˜ ê²Œì‹œê¸€ë§Œ ìˆ˜ì§‘
start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
end_date = datetime.now().strftime('%Y-%m-%d')

posts = crawler.crawl_keyword_posts(
    keyword="ê²€ìƒ‰ì–´",
    start_date=start_date,
    end_date=end_date
)
```

### ì»¤ìŠ¤í…€ ë¶„ì„
```python
# ë¸”ë¡œê·¸ë³„ ê²Œì‹œê¸€ ìˆ˜ ë¶„ì„
blog_counts = {}
for post in posts:
    blog_name = post.get('blog_name', 'Unknown')
    blog_counts[blog_name] = blog_counts.get(blog_name, 0) + 1

# ì¢‹ì•„ìš” ìˆ˜ ìƒìœ„ ê²Œì‹œê¸€
top_liked = sorted(posts, key=lambda x: x.get('like_count', 0), reverse=True)[:10]
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ì†ë„ ì œí•œ**: ì„œë²„ ë¶€í•˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì ì ˆí•œ ì§€ì—° ì‹œê°„ì„ ì„¤ì •í•˜ì„¸ìš”.
2. **ë¡œë´‡ ë°°ì œ í‘œì¤€**: ê° ë¸”ë¡œê·¸ì˜ robots.txtë¥¼ í™•ì¸í•˜ê³  ì¤€ìˆ˜í•˜ì„¸ìš”.
3. **ì´ìš© ì•½ê´€**: í‹°ìŠ¤í† ë¦¬ ì´ìš©ì•½ê´€ì„ ì¤€ìˆ˜í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.
4. **ê°œì¸ì •ë³´**: ìˆ˜ì§‘ëœ ë°ì´í„°ì— ê°œì¸ì •ë³´ê°€ í¬í•¨ë  ìˆ˜ ìˆìœ¼ë‹ˆ ì£¼ì˜í•˜ì„¸ìš”.
5. **Chrome ë¸Œë¼ìš°ì €**: Selenium ë²„ì „ ì‚¬ìš© ì‹œ Chrome ë¸Œë¼ìš°ì €ê°€ í•„ìš”í•©ë‹ˆë‹¤.

## ğŸ› ë¬¸ì œ í•´ê²°

### Chrome WebDriver ì˜¤ë¥˜
```bash
# Chrome ë¸Œë¼ìš°ì € ì„¤ì¹˜ í™•ì¸
google-chrome --version

# ìˆ˜ë™ìœ¼ë¡œ ChromeDriver ì„¤ì¹˜
pip install webdriver-manager
```

### ì¸ì½”ë”© ì˜¤ë¥˜
```python
# CSV íŒŒì¼ì„ Excelì—ì„œ ì—´ ë•Œ í•œê¸€ì´ ê¹¨ì§€ëŠ” ê²½ìš°
df.to_csv("output.csv", index=False, encoding='utf-8-sig')
```

### ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ ë°°ì¹˜ ì²˜ë¦¬
def process_in_batches(posts, batch_size=100):
    for i in range(0, len(posts), batch_size):
        batch = posts[i:i+batch_size]
        # ë°°ì¹˜ ì²˜ë¦¬ ë¡œì§
```

## ğŸ“ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ ì§€ì›

ë¬¸ì œê°€ ë°œìƒí•˜ê±°ë‚˜ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì´ìŠˆë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.