#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Tistory Blog Crawler
í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ - í‚¤ì›Œë“œ ê²€ìƒ‰ì„ í†µí•œ ê²Œì‹œê¸€, ëŒ“ê¸€, ì¢‹ì•„ìš” ìˆ˜ ìˆ˜ì§‘
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse, parse_qs
from fake_useragent import UserAgent
import json
import logging
from typing import List, Dict, Optional
from tqdm import tqdm
import os

class TistoryCrawler:
    def __init__(self, delay: float = 1.0):
        """
        í‹°ìŠ¤í† ë¦¬ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            delay: ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ)
        """
        self.session = requests.Session()
        self.ua = UserAgent()
        self.delay = delay
        self.logger = self._setup_logger()
        
        # ê¸°ë³¸ í—¤ë” ì„¤ì •
        self.session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def _setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger('TistoryCrawler')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _make_request(self, url: str, params: Dict = None) -> Optional[BeautifulSoup]:
        """
        HTTP ìš”ì²­ ë° BeautifulSoup ê°ì²´ ë°˜í™˜
        
        Args:
            url: ìš”ì²­í•  URL
            params: ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°
            
        Returns:
            BeautifulSoup ê°ì²´ ë˜ëŠ” None
        """
        try:
            time.sleep(self.delay)
            response = self.session.get(url, params=params, timeout=10)
            response.raise_for_status()
            
            # ì¸ì½”ë”© ì„¤ì •
            if response.encoding == 'ISO-8859-1':
                response.encoding = 'utf-8'
            
            return BeautifulSoup(response.text, 'lxml')
            
        except Exception as e:
            self.logger.error(f"ìš”ì²­ ì‹¤íŒ¨: {url} - {str(e)}")
            return None
    
    def search_tistory_posts(self, keyword: str, start_date: str = None, end_date: str = None, 
                           max_pages: int = 10) -> List[Dict]:
        """
        í‹°ìŠ¤í† ë¦¬ ê²€ìƒ‰ì„ í†µí•œ ê²Œì‹œê¸€ ìˆ˜ì§‘
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
            end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            
        Returns:
            ê²Œì‹œê¸€ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        posts = []
        search_url = "https://www.tistory.com/search"
        
        self.logger.info(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹œì‘")
        
        for page in range(1, max_pages + 1):
            self.logger.info(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")
            
            params = {
                'keyword': keyword,
                'page': page
            }
            
            soup = self._make_request(search_url, params)
            if not soup:
                break
            
            # ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
            post_items = soup.find_all('div', class_='item_post')
            
            if not post_items:
                self.logger.info(f"í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            for item in post_items:
                post_info = self._parse_search_result(item, start_date, end_date)
                if post_info:
                    posts.append(post_info)
            
            # ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
            if not self._has_next_page(soup):
                break
        
        self.logger.info(f"ì´ {len(posts)}ê°œì˜ ê²Œì‹œê¸€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return posts
    
    def _parse_search_result(self, item, start_date: str = None, end_date: str = None) -> Optional[Dict]:
        """
        ê²€ìƒ‰ ê²°ê³¼ ì•„ì´í…œ íŒŒì‹±
        
        Args:
            item: BeautifulSoup ê²€ìƒ‰ ê²°ê³¼ ì•„ì´í…œ
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ
            
        Returns:
            ê²Œì‹œê¸€ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # ì œëª©ê³¼ ë§í¬
            title_elem = item.find('a', class_='link_post')
            if not title_elem:
                return None
            
            title = title_elem.get_text(strip=True)
            post_url = title_elem.get('href')
            
            # ë¸”ë¡œê·¸ ì •ë³´
            blog_elem = item.find('a', class_='link_blog')
            blog_name = blog_elem.get_text(strip=True) if blog_elem else "Unknown"
            blog_url = blog_elem.get('href') if blog_elem else ""
            
            # ë‚ ì§œ ì •ë³´
            date_elem = item.find('span', class_='txt_date')
            post_date = date_elem.get_text(strip=True) if date_elem else ""
            
            # ë‚ ì§œ í•„í„°ë§
            if start_date or end_date:
                if not self._is_date_in_range(post_date, start_date, end_date):
                    return None
            
            # ë¯¸ë¦¬ë³´ê¸° í…ìŠ¤íŠ¸
            preview_elem = item.find('p', class_='txt_post')
            preview = preview_elem.get_text(strip=True) if preview_elem else ""
            
            return {
                'title': title,
                'url': post_url,
                'blog_name': blog_name,
                'blog_url': blog_url,
                'date': post_date,
                'preview': preview,
                'crawled_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _has_next_page(self, soup: BeautifulSoup) -> bool:
        """ë‹¤ìŒ í˜ì´ì§€ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        next_btn = soup.find('a', class_='btn_next')
        return next_btn is not None and 'disabled' not in next_btn.get('class', [])
    
    def _is_date_in_range(self, date_str: str, start_date: str = None, end_date: str = None) -> bool:
        """ë‚ ì§œê°€ ì§€ì •ëœ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸"""
        try:
            # í‹°ìŠ¤í† ë¦¬ ë‚ ì§œ í˜•ì‹ íŒŒì‹± (ì˜ˆ: "2024.01.15", "1ì¼ ì „", "1ì£¼ ì „" ë“±)
            post_date = self._parse_tistory_date(date_str)
            if not post_date:
                return True  # ë‚ ì§œë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìœ¼ë©´ í¬í•¨
            
            if start_date:
                start_dt = datetime.strptime(start_date, '%Y-%m-%d')
                if post_date < start_dt:
                    return False
            
            if end_date:
                end_dt = datetime.strptime(end_date, '%Y-%m-%d')
                if post_date > end_dt:
                    return False
            
            return True
            
        except Exception:
            return True  # ì˜¤ë¥˜ ì‹œ í¬í•¨
    
    def _parse_tistory_date(self, date_str: str) -> Optional[datetime]:
        """í‹°ìŠ¤í† ë¦¬ ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜"""
        try:
            # "YYYY.MM.DD" í˜•ì‹
            if re.match(r'\d{4}\.\d{1,2}\.\d{1,2}', date_str):
                return datetime.strptime(date_str, '%Y.%m.%d')
            
            # "Nì¼ ì „", "Nì£¼ ì „", "Nê°œì›” ì „" í˜•ì‹
            now = datetime.now()
            
            if 'ì¼ ì „' in date_str:
                days = int(re.search(r'(\d+)ì¼ ì „', date_str).group(1))
                return now - timedelta(days=days)
            elif 'ì£¼ ì „' in date_str:
                weeks = int(re.search(r'(\d+)ì£¼ ì „', date_str).group(1))
                return now - timedelta(weeks=weeks)
            elif 'ê°œì›” ì „' in date_str:
                months = int(re.search(r'(\d+)ê°œì›” ì „', date_str).group(1))
                return now - timedelta(days=months * 30)
            elif 'ë…„ ì „' in date_str:
                years = int(re.search(r'(\d+)ë…„ ì „', date_str).group(1))
                return now - timedelta(days=years * 365)
            
            return None
            
        except Exception:
            return None
    
    def crawl_post_details(self, post_url: str) -> Dict:
        """
        ê°œë³„ ê²Œì‹œê¸€ì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
        
        Args:
            post_url: ê²Œì‹œê¸€ URL
            
        Returns:
            ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´
        """
        soup = self._make_request(post_url)
        if not soup:
            return {}
        
        try:
            # ê²Œì‹œê¸€ ë‚´ìš©
            content_elem = soup.find('div', class_='entry-content') or soup.find('div', class_='tt_article_useless_p_margin')
            content = content_elem.get_text(strip=True) if content_elem else ""
            
            # ì¢‹ì•„ìš”/ê³µê° ìˆ˜
            like_count = self._extract_like_count(soup)
            
            # ëŒ“ê¸€ ìˆ˜ ë° ëŒ“ê¸€ ì •ë³´
            comments_info = self._extract_comments(soup, post_url)
            
            # ì¡°íšŒìˆ˜
            view_count = self._extract_view_count(soup)
            
            return {
                'content': content,
                'like_count': like_count,
                'view_count': view_count,
                'comment_count': comments_info['count'],
                'comments': comments_info['comments']
            }
            
        except Exception as e:
            self.logger.error(f"ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì˜¤ë¥˜: {post_url} - {str(e)}")
            return {}
    
    def _extract_like_count(self, soup: BeautifulSoup) -> int:
        """ì¢‹ì•„ìš”/ê³µê° ìˆ˜ ì¶”ì¶œ"""
        try:
            # ë‹¤ì–‘í•œ ì¢‹ì•„ìš” ë²„íŠ¼ ì…€ë ‰í„° ì‹œë„
            selectors = [
                '.btn_sympathy .txt_num',
                '.area_sympathy .txt_num',
                '.btn_like .num',
                '.like_count',
                '.sympathy_count'
            ]
            
            for selector in selectors:
                elem = soup.select_one(selector)
                if elem:
                    count_text = elem.get_text(strip=True)
                    return int(re.search(r'\d+', count_text).group()) if re.search(r'\d+', count_text) else 0
            
            return 0
            
        except Exception:
            return 0
    
    def _extract_view_count(self, soup: BeautifulSoup) -> int:
        """ì¡°íšŒìˆ˜ ì¶”ì¶œ"""
        try:
            # ì¡°íšŒìˆ˜ ê´€ë ¨ ì…€ë ‰í„°
            selectors = [
                '.view_count',
                '.cnt_view',
                '.num_view'
            ]
            
            for selector in selectors:
                elem = soup.select_one(selector)
                if elem:
                    count_text = elem.get_text(strip=True)
                    return int(re.search(r'\d+', count_text).group()) if re.search(r'\d+', count_text) else 0
            
            return 0
            
        except Exception:
            return 0
    
    def _extract_comments(self, soup: BeautifulSoup, post_url: str) -> Dict:
        """ëŒ“ê¸€ ì •ë³´ ì¶”ì¶œ"""
        try:
            comments = []
            
            # ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            comment_containers = soup.find_all(['div', 'li'], class_=re.compile(r'comment|reply'))
            
            for container in comment_containers:
                comment_info = self._parse_comment(container)
                if comment_info:
                    comments.append(comment_info)
            
            # ëŒ“ê¸€ì´ AJAXë¡œ ë¡œë“œë˜ëŠ” ê²½ìš°ë¥¼ ìœ„í•œ ì¶”ê°€ ì²˜ë¦¬
            if not comments:
                comments = self._extract_ajax_comments(post_url)
            
            return {
                'count': len(comments),
                'comments': comments
            }
            
        except Exception as e:
            self.logger.error(f"ëŒ“ê¸€ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return {'count': 0, 'comments': []}
    
    def _parse_comment(self, container) -> Optional[Dict]:
        """ê°œë³„ ëŒ“ê¸€ íŒŒì‹±"""
        try:
            # ì‘ì„±ì
            author_elem = container.find(['span', 'strong'], class_=re.compile(r'name|author|nick'))
            author = author_elem.get_text(strip=True) if author_elem else "ìµëª…"
            
            # ëŒ“ê¸€ ë‚´ìš©
            content_elem = container.find(['p', 'div'], class_=re.compile(r'content|text|comment'))
            content = content_elem.get_text(strip=True) if content_elem else ""
            
            # ë‚ ì§œ
            date_elem = container.find(['span', 'time'], class_=re.compile(r'date|time'))
            date = date_elem.get_text(strip=True) if date_elem else ""
            
            if not content:
                return None
            
            return {
                'author': author,
                'content': content,
                'date': date
            }
            
        except Exception:
            return None
    
    def _extract_ajax_comments(self, post_url: str) -> List[Dict]:
        """AJAXë¡œ ë¡œë“œë˜ëŠ” ëŒ“ê¸€ ì¶”ì¶œ (ê¸°ë³¸ êµ¬í˜„)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê° ë¸”ë¡œê·¸ì˜ ëŒ“ê¸€ APIë¥¼ ë¶„ì„í•´ì•¼ í•¨
        return []
    
    def crawl_keyword_posts(self, keyword: str, start_date: str = None, end_date: str = None,
                          max_pages: int = 10, include_details: bool = True) -> List[Dict]:
        """
        í‚¤ì›Œë“œ ê¸°ë°˜ ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
            end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            include_details: ìƒì„¸ ì •ë³´ í¬í•¨ ì—¬ë¶€
            
        Returns:
            ì™„ì „í•œ ê²Œì‹œê¸€ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        # 1ë‹¨ê³„: ê²€ìƒ‰ì„ í†µí•œ ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘
        posts = self.search_tistory_posts(keyword, start_date, end_date, max_pages)
        
        if not include_details:
            return posts
        
        # 2ë‹¨ê³„: ê° ê²Œì‹œê¸€ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
        self.logger.info("ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        
        for i, post in enumerate(tqdm(posts, desc="ìƒì„¸ ì •ë³´ í¬ë¡¤ë§")):
            try:
                details = self.crawl_post_details(post['url'])
                post.update(details)
                
                # ì§„í–‰ë¥  ë¡œê·¸
                if (i + 1) % 10 == 0:
                    self.logger.info(f"{i + 1}/{len(posts)} ê²Œì‹œê¸€ ì²˜ë¦¬ ì™„ë£Œ")
                    
            except Exception as e:
                self.logger.error(f"ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {post['url']} - {str(e)}")
                continue
        
        return posts
    
    def save_to_csv(self, posts: List[Dict], filename: str = None):
        """ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'tistory_crawl_{timestamp}.csv'
        
        # ëŒ“ê¸€ ì •ë³´ë¥¼ ë³„ë„ ì²˜ë¦¬
        flattened_data = []
        
        for post in posts:
            base_data = {
                'title': post.get('title', ''),
                'url': post.get('url', ''),
                'blog_name': post.get('blog_name', ''),
                'blog_url': post.get('blog_url', ''),
                'date': post.get('date', ''),
                'content': post.get('content', ''),
                'like_count': post.get('like_count', 0),
                'view_count': post.get('view_count', 0),
                'comment_count': post.get('comment_count', 0),
                'preview': post.get('preview', ''),
                'crawled_at': post.get('crawled_at', '')
            }
            
            # ëŒ“ê¸€ì´ ìˆëŠ” ê²½ìš° ê° ëŒ“ê¸€ì„ ë³„ë„ í–‰ìœ¼ë¡œ ì¶”ê°€
            comments = post.get('comments', [])
            if comments:
                for comment in comments:
                    row_data = base_data.copy()
                    row_data.update({
                        'comment_author': comment.get('author', ''),
                        'comment_content': comment.get('content', ''),
                        'comment_date': comment.get('date', '')
                    })
                    flattened_data.append(row_data)
            else:
                # ëŒ“ê¸€ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ë°ì´í„°ë§Œ ì¶”ê°€
                base_data.update({
                    'comment_author': '',
                    'comment_content': '',
                    'comment_date': ''
                })
                flattened_data.append(base_data)
        
        df = pd.DataFrame(flattened_data)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        self.logger.info(f"ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return filename
    
    def save_to_json(self, posts: List[Dict], filename: str = None):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'tistory_crawl_{timestamp}.json'
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(posts, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return filename


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    crawler = TistoryCrawler(delay=1.5)  # 1.5ì´ˆ ì§€ì—°
    
    # ê²€ìƒ‰ ì„¤ì •
    keyword = input("ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
    start_date = input("ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD, ì—”í„°ë¡œ ìŠ¤í‚µ): ").strip() or None
    end_date = input("ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD, ì—”í„°ë¡œ ìŠ¤í‚µ): ").strip() or None
    max_pages = int(input("ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 5): ") or 5)
    
    print(f"\ní‚¤ì›Œë“œ '{keyword}' í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"ê¸°ê°„: {start_date or 'ì œí•œì—†ìŒ'} ~ {end_date or 'ì œí•œì—†ìŒ'}")
    print(f"ìµœëŒ€ í˜ì´ì§€: {max_pages}")
    print("-" * 50)
    
    try:
        # í¬ë¡¤ë§ ì‹¤í–‰
        posts = crawler.crawl_keyword_posts(
            keyword=keyword,
            start_date=start_date,
            end_date=end_date,
            max_pages=max_pages,
            include_details=True
        )
        
        if posts:
            print(f"\nâœ… ì´ {len(posts)}ê°œì˜ ê²Œì‹œê¸€ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            
            # ê²°ê³¼ ì €ì¥
            csv_file = crawler.save_to_csv(posts)
            json_file = crawler.save_to_json(posts)
            
            print(f"ğŸ“„ CSV íŒŒì¼: {csv_file}")
            print(f"ğŸ“„ JSON íŒŒì¼: {json_file}")
            
            # ê°„ë‹¨í•œ í†µê³„
            total_comments = sum(post.get('comment_count', 0) for post in posts)
            total_likes = sum(post.get('like_count', 0) for post in posts)
            
            print(f"\nğŸ“Š ìˆ˜ì§‘ í†µê³„:")
            print(f"   - ì´ ëŒ“ê¸€ ìˆ˜: {total_comments:,}")
            print(f"   - ì´ ì¢‹ì•„ìš” ìˆ˜: {total_likes:,}")
            
        else:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


if __name__ == "__main__":
    main()