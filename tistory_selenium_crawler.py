#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Tistory Blog Crawler with Selenium
í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ (Selenium ë²„ì „) - ë™ì  ì½˜í…ì¸  ì§€ì›
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager

import pandas as pd
import time
import re
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse
import json
import logging
from typing import List, Dict, Optional
from tqdm import tqdm
import os

class TistorySeleniumCrawler:
    def __init__(self, headless: bool = True, delay: float = 2.0):
        """
        í‹°ìŠ¤í† ë¦¬ Selenium í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€
            delay: ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ)
        """
        self.delay = delay
        self.logger = self._setup_logger()
        self.driver = self._setup_driver(headless)
        
    def _setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger('TistorySeleniumCrawler')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _setup_driver(self, headless: bool) -> webdriver.Chrome:
        """Chrome WebDriver ì„¤ì •"""
        try:
            chrome_options = Options()
            
            if headless:
                chrome_options.add_argument('--headless')
            
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            # ì´ë¯¸ì§€, CSS ë“± ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨ìœ¼ë¡œ ì†ë„ í–¥ìƒ
            prefs = {
                "profile.managed_default_content_settings.images": 2,
                "profile.default_content_setting_values": {
                    "notifications": 2
                }
            }
            chrome_options.add_experimental_option("prefs", prefs)
            
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
            driver.implicitly_wait(10)
            
            self.logger.info("Chrome WebDriver ì´ˆê¸°í™” ì™„ë£Œ")
            return driver
            
        except Exception as e:
            self.logger.error(f"WebDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise
    
    def __del__(self):
        """ì†Œë©¸ì - WebDriver ì¢…ë£Œ"""
        if hasattr(self, 'driver'):
            try:
                self.driver.quit()
            except:
                pass
    
    def search_tistory_posts(self, keyword: str, start_date: str = None, end_date: str = None, 
                           max_pages: int = 10) -> List[Dict]:
        """
        í‹°ìŠ¤í† ë¦¬ ê²€ìƒ‰ì„ í†µí•œ ê²Œì‹œê¸€ ìˆ˜ì§‘ (Selenium ë²„ì „)
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
            end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            
        Returns:
            ê²Œì‹œê¸€ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        posts = []
        search_url = f"https://www.tistory.com/search?keyword={keyword}"
        
        self.logger.info(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹œì‘")
        
        try:
            self.driver.get(search_url)
            time.sleep(self.delay)
            
            for page in range(1, max_pages + 1):
                self.logger.info(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")
                
                # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                try:
                    WebDriverWait(self.driver, 10).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, ".item_post, .list_post li"))
                    )
                except TimeoutException:
                    self.logger.warning(f"í˜ì´ì§€ {page} ë¡œë”© íƒ€ì„ì•„ì›ƒ")
                    break
                
                # ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
                post_elements = self.driver.find_elements(By.CSS_SELECTOR, ".item_post, .list_post li")
                
                if not post_elements:
                    self.logger.info(f"í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                page_posts = []
                for element in post_elements:
                    post_info = self._parse_search_result_selenium(element, start_date, end_date)
                    if post_info:
                        page_posts.append(post_info)
                
                posts.extend(page_posts)
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                if page < max_pages and not self._go_to_next_page():
                    break
                
                time.sleep(self.delay)
            
        except Exception as e:
            self.logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        self.logger.info(f"ì´ {len(posts)}ê°œì˜ ê²Œì‹œê¸€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return posts
    
    def _parse_search_result_selenium(self, element, start_date: str = None, end_date: str = None) -> Optional[Dict]:
        """
        ê²€ìƒ‰ ê²°ê³¼ ì•„ì´í…œ íŒŒì‹± (Selenium ë²„ì „)
        """
        try:
            # ì œëª©ê³¼ ë§í¬
            title_elem = element.find_element(By.CSS_SELECTOR, "a[href*='tistory.com'], .link_post, .tit_post a")
            title = title_elem.text.strip()
            post_url = title_elem.get_attribute('href')
            
            if not title or not post_url:
                return None
            
            # ë¸”ë¡œê·¸ ì •ë³´
            try:
                blog_elem = element.find_element(By.CSS_SELECTOR, ".link_blog, .blog_name a, .name_blog")
                blog_name = blog_elem.text.strip()
                blog_url = blog_elem.get_attribute('href') or ""
            except NoSuchElementException:
                blog_name = "Unknown"
                blog_url = ""
            
            # ë‚ ì§œ ì •ë³´
            try:
                date_elem = element.find_element(By.CSS_SELECTOR, ".txt_date, .date, .info_post .date")
                post_date = date_elem.text.strip()
            except NoSuchElementException:
                post_date = ""
            
            # ë‚ ì§œ í•„í„°ë§
            if start_date or end_date:
                if not self._is_date_in_range(post_date, start_date, end_date):
                    return None
            
            # ë¯¸ë¦¬ë³´ê¸° í…ìŠ¤íŠ¸
            try:
                preview_elem = element.find_element(By.CSS_SELECTOR, ".txt_post, .desc_post, .summary")
                preview = preview_elem.text.strip()
            except NoSuchElementException:
                preview = ""
            
            return {
                'title': title,
                'url': post_url,
                'blog_name': blog_name,
                'blog_url': blog_url,
                'date': post_date,
                'preview': preview,
                'crawled_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _go_to_next_page(self) -> bool:
        """ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™"""
        try:
            next_btn = self.driver.find_element(By.CSS_SELECTOR, ".btn_next, .next, a[href*='page=']")
            if next_btn and next_btn.is_enabled():
                self.driver.execute_script("arguments[0].click();", next_btn)
                time.sleep(self.delay)
                return True
            return False
        except NoSuchElementException:
            return False
    
    def crawl_post_details(self, post_url: str) -> Dict:
        """
        ê°œë³„ ê²Œì‹œê¸€ì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ (Selenium ë²„ì „)
        
        Args:
            post_url: ê²Œì‹œê¸€ URL
            
        Returns:
            ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´
        """
        try:
            self.driver.get(post_url)
            time.sleep(self.delay)
            
            # í˜ì´ì§€ ë¡œë”© ì™„ë£Œ ëŒ€ê¸°
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° (ëŒ“ê¸€ ë“±)
            time.sleep(2)
            
            # ê²Œì‹œê¸€ ë‚´ìš©
            content = self._extract_content_selenium()
            
            # ì¢‹ì•„ìš”/ê³µê° ìˆ˜
            like_count = self._extract_like_count_selenium()
            
            # ì¡°íšŒìˆ˜
            view_count = self._extract_view_count_selenium()
            
            # ëŒ“ê¸€ ìˆ˜ ë° ëŒ“ê¸€ ì •ë³´
            comments_info = self._extract_comments_selenium()
            
            return {
                'content': content,
                'like_count': like_count,
                'view_count': view_count,
                'comment_count': comments_info['count'],
                'comments': comments_info['comments']
            }
            
        except Exception as e:
            self.logger.error(f"ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì˜¤ë¥˜: {post_url} - {str(e)}")
            return {
                'content': '',
                'like_count': 0,
                'view_count': 0,
                'comment_count': 0,
                'comments': []
            }
    
    def _extract_content_selenium(self) -> str:
        """ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ (Selenium)"""
        try:
            content_selectors = [
                ".entry-content",
                ".tt_article_useless_p_margin",
                ".contents_style",
                "#content",
                ".post_content",
                ".article_view"
            ]
            
            for selector in content_selectors:
                try:
                    content_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    return content_elem.text.strip()
                except NoSuchElementException:
                    continue
            
            return ""
            
        except Exception:
            return ""
    
    def _extract_like_count_selenium(self) -> int:
        """ì¢‹ì•„ìš”/ê³µê° ìˆ˜ ì¶”ì¶œ (Selenium)"""
        try:
            like_selectors = [
                ".btn_sympathy .txt_num",
                ".area_sympathy .txt_num",
                ".btn_like .num",
                ".like_count",
                ".sympathy_count",
                "[class*='sympathy'] [class*='count']",
                "[class*='like'] [class*='count']"
            ]
            
            for selector in like_selectors:
                try:
                    elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    count_text = elem.text.strip()
                    match = re.search(r'\d+', count_text)
                    if match:
                        return int(match.group())
                except NoSuchElementException:
                    continue
            
            return 0
            
        except Exception:
            return 0
    
    def _extract_view_count_selenium(self) -> int:
        """ì¡°íšŒìˆ˜ ì¶”ì¶œ (Selenium)"""
        try:
            view_selectors = [
                ".view_count",
                ".cnt_view",
                ".num_view",
                "[class*='view'] [class*='count']"
            ]
            
            for selector in view_selectors:
                try:
                    elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    count_text = elem.text.strip()
                    match = re.search(r'\d+', count_text)
                    if match:
                        return int(match.group())
                except NoSuchElementException:
                    continue
            
            return 0
            
        except Exception:
            return 0
    
    def _extract_comments_selenium(self) -> Dict:
        """ëŒ“ê¸€ ì •ë³´ ì¶”ì¶œ (Selenium)"""
        try:
            comments = []
            
            # ëŒ“ê¸€ ë¡œë”© ëŒ€ê¸° ë° ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­
            self._load_all_comments()
            
            # ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            comment_selectors = [
                ".comment_list li",
                ".guestbook_list li",
                "[class*='comment'] li",
                ".reply_list li"
            ]
            
            comment_elements = []
            for selector in comment_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        comment_elements = elements
                        break
                except:
                    continue
            
            for element in comment_elements:
                comment_info = self._parse_comment_selenium(element)
                if comment_info:
                    comments.append(comment_info)
            
            return {
                'count': len(comments),
                'comments': comments
            }
            
        except Exception as e:
            self.logger.error(f"ëŒ“ê¸€ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return {'count': 0, 'comments': []}
    
    def _load_all_comments(self):
        """ëª¨ë“  ëŒ“ê¸€ ë¡œë”© (ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­)"""
        try:
            # ëŒ“ê¸€ ë”ë³´ê¸° ë²„íŠ¼ë“¤
            more_btn_selectors = [
                ".btn_more_comment",
                ".more_comment",
                "[class*='more'][class*='comment']",
                ".btn_more"
            ]
            
            max_clicks = 10  # ë¬´í•œ ë£¨í”„ ë°©ì§€
            clicks = 0
            
            while clicks < max_clicks:
                clicked = False
                
                for selector in more_btn_selectors:
                    try:
                        btn = self.driver.find_element(By.CSS_SELECTOR, selector)
                        if btn.is_displayed() and btn.is_enabled():
                            self.driver.execute_script("arguments[0].click();", btn)
                            time.sleep(1)
                            clicked = True
                            clicks += 1
                            break
                    except:
                        continue
                
                if not clicked:
                    break
                    
        except Exception:
            pass
    
    def _parse_comment_selenium(self, element) -> Optional[Dict]:
        """ê°œë³„ ëŒ“ê¸€ íŒŒì‹± (Selenium)"""
        try:
            # ì‘ì„±ì
            author_selectors = [
                ".name",
                ".author",
                ".nick",
                "[class*='name']",
                "[class*='author']"
            ]
            
            author = "ìµëª…"
            for selector in author_selectors:
                try:
                    author_elem = element.find_element(By.CSS_SELECTOR, selector)
                    author = author_elem.text.strip()
                    if author:
                        break
                except:
                    continue
            
            # ëŒ“ê¸€ ë‚´ìš©
            content_selectors = [
                ".comment_content",
                ".content",
                ".text",
                "[class*='content']",
                "[class*='text']"
            ]
            
            content = ""
            for selector in content_selectors:
                try:
                    content_elem = element.find_element(By.CSS_SELECTOR, selector)
                    content = content_elem.text.strip()
                    if content:
                        break
                except:
                    continue
            
            # ë‚ ì§œ
            date_selectors = [
                ".date",
                ".time",
                "[class*='date']",
                "[class*='time']"
            ]
            
            date = ""
            for selector in date_selectors:
                try:
                    date_elem = element.find_element(By.CSS_SELECTOR, selector)
                    date = date_elem.text.strip()
                    if date:
                        break
                except:
                    continue
            
            if not content:
                return None
            
            return {
                'author': author,
                'content': content,
                'date': date
            }
            
        except Exception:
            return None
    
    def _is_date_in_range(self, date_str: str, start_date: str = None, end_date: str = None) -> bool:
        """ë‚ ì§œê°€ ì§€ì •ëœ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸"""
        try:
            post_date = self._parse_tistory_date(date_str)
            if not post_date:
                return True
            
            if start_date:
                start_dt = datetime.strptime(start_date, '%Y-%m-%d')
                if post_date < start_dt:
                    return False
            
            if end_date:
                end_dt = datetime.strptime(end_date, '%Y-%m-%d')
                if post_date > end_dt:
                    return False
            
            return True
            
        except Exception:
            return True
    
    def _parse_tistory_date(self, date_str: str) -> Optional[datetime]:
        """í‹°ìŠ¤í† ë¦¬ ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜"""
        try:
            if re.match(r'\d{4}\.\d{1,2}\.\d{1,2}', date_str):
                return datetime.strptime(date_str, '%Y.%m.%d')
            
            now = datetime.now()
            
            if 'ì¼ ì „' in date_str:
                days = int(re.search(r'(\d+)ì¼ ì „', date_str).group(1))
                return now - timedelta(days=days)
            elif 'ì£¼ ì „' in date_str:
                weeks = int(re.search(r'(\d+)ì£¼ ì „', date_str).group(1))
                return now - timedelta(weeks=weeks)
            elif 'ê°œì›” ì „' in date_str:
                months = int(re.search(r'(\d+)ê°œì›” ì „', date_str).group(1))
                return now - timedelta(days=months * 30)
            elif 'ë…„ ì „' in date_str:
                years = int(re.search(r'(\d+)ë…„ ì „', date_str).group(1))
                return now - timedelta(days=years * 365)
            
            return None
            
        except Exception:
            return None
    
    def crawl_keyword_posts(self, keyword: str, start_date: str = None, end_date: str = None,
                          max_pages: int = 10, include_details: bool = True) -> List[Dict]:
        """
        í‚¤ì›Œë“œ ê¸°ë°˜ ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ (Selenium ë²„ì „)
        """
        posts = self.search_tistory_posts(keyword, start_date, end_date, max_pages)
        
        if not include_details or not posts:
            return posts
        
        self.logger.info("ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        
        for i, post in enumerate(tqdm(posts, desc="ìƒì„¸ ì •ë³´ í¬ë¡¤ë§")):
            try:
                details = self.crawl_post_details(post['url'])
                post.update(details)
                
                if (i + 1) % 5 == 0:
                    self.logger.info(f"{i + 1}/{len(posts)} ê²Œì‹œê¸€ ì²˜ë¦¬ ì™„ë£Œ")
                    
            except Exception as e:
                self.logger.error(f"ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {post['url']} - {str(e)}")
                continue
        
        return posts
    
    def save_to_csv(self, posts: List[Dict], filename: str = None) -> str:
        """ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'tistory_selenium_crawl_{timestamp}.csv'
        
        flattened_data = []
        
        for post in posts:
            base_data = {
                'title': post.get('title', ''),
                'url': post.get('url', ''),
                'blog_name': post.get('blog_name', ''),
                'blog_url': post.get('blog_url', ''),
                'date': post.get('date', ''),
                'content': post.get('content', ''),
                'like_count': post.get('like_count', 0),
                'view_count': post.get('view_count', 0),
                'comment_count': post.get('comment_count', 0),
                'preview': post.get('preview', ''),
                'crawled_at': post.get('crawled_at', '')
            }
            
            comments = post.get('comments', [])
            if comments:
                for comment in comments:
                    row_data = base_data.copy()
                    row_data.update({
                        'comment_author': comment.get('author', ''),
                        'comment_content': comment.get('content', ''),
                        'comment_date': comment.get('date', '')
                    })
                    flattened_data.append(row_data)
            else:
                base_data.update({
                    'comment_author': '',
                    'comment_content': '',
                    'comment_date': ''
                })
                flattened_data.append(base_data)
        
        df = pd.DataFrame(flattened_data)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        self.logger.info(f"ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return filename
    
    def save_to_json(self, posts: List[Dict], filename: str = None) -> str:
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'tistory_selenium_crawl_{timestamp}.json'
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(posts, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return filename


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Tistory Selenium í¬ë¡¤ëŸ¬")
    print("=" * 50)
    
    # í¬ë¡¤ëŸ¬ ì„¤ì •
    headless = input("í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‚¬ìš©? (y/N): ").lower().startswith('y')
    crawler = TistorySeleniumCrawler(headless=headless, delay=2.0)
    
    try:
        # ê²€ìƒ‰ ì„¤ì •
        keyword = input("ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
        start_date = input("ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD, ì—”í„°ë¡œ ìŠ¤í‚µ): ").strip() or None
        end_date = input("ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD, ì—”í„°ë¡œ ìŠ¤í‚µ): ").strip() or None
        max_pages = int(input("ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 3): ") or 3)
        
        print(f"\nğŸ” í‚¤ì›Œë“œ '{keyword}' í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        print(f"ğŸ“… ê¸°ê°„: {start_date or 'ì œí•œì—†ìŒ'} ~ {end_date or 'ì œí•œì—†ìŒ'}")
        print(f"ğŸ“„ ìµœëŒ€ í˜ì´ì§€: {max_pages}")
        print("-" * 50)
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        posts = crawler.crawl_keyword_posts(
            keyword=keyword,
            start_date=start_date,
            end_date=end_date,
            max_pages=max_pages,
            include_details=True
        )
        
        if posts:
            print(f"\nâœ… ì´ {len(posts)}ê°œì˜ ê²Œì‹œê¸€ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            
            # ê²°ê³¼ ì €ì¥
            csv_file = crawler.save_to_csv(posts)
            json_file = crawler.save_to_json(posts)
            
            print(f"ğŸ“„ CSV íŒŒì¼: {csv_file}")
            print(f"ğŸ“„ JSON íŒŒì¼: {json_file}")
            
            # í†µê³„
            total_comments = sum(post.get('comment_count', 0) for post in posts)
            total_likes = sum(post.get('like_count', 0) for post in posts)
            total_views = sum(post.get('view_count', 0) for post in posts)
            
            print(f"\nğŸ“Š ìˆ˜ì§‘ í†µê³„:")
            print(f"   - ì´ ëŒ“ê¸€ ìˆ˜: {total_comments:,}")
            print(f"   - ì´ ì¢‹ì•„ìš” ìˆ˜: {total_likes:,}")
            print(f"   - ì´ ì¡°íšŒìˆ˜: {total_views:,}")
            
        else:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    finally:
        # WebDriver ì •ë¦¬
        try:
            crawler.driver.quit()
        except:
            pass


if __name__ == "__main__":
    main()